{
 "metadata": {
  "name": "",
  "signature": "sha256:d7776207390fc6503661d62ff373a479e35ed3e07155174044a3bf6ab1e06334"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analyzing Personal Characteristics and Work Environment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import sys\n",
      "import string\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from os import path, pardir\n",
      "from string import punctuation\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load in Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data =  pd.io.pickle.read_pickle(path.join(pardir, 'data', 'preprocessed_text_dec14.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Flat array of sentences for testing\n",
      "tagged_sents_flat = [item for sublist in data.ngram_tags.values for item in sublist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 192
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tried using WordNet to identify nouns that refer to people "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "person_words = []\n",
      "# Find words that are hyponyms of person\n",
      "for synset in wn.synsets('person'):\n",
      "    for i in xrange(len(synset.hyponyms())):\n",
      "        person_words.append(synset.hyponyms()[i].lemma_names)\n",
      "person_set = set([item for sublist in person_words for item in sublist])\n",
      "len(person_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 193,
       "text": [
        "629"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Results are too broad, but some of highest frequency terms (like candidate) could be helpful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "person_nouns = []\n",
      "for sent in tagged_sents_flat:\n",
      "    for word, tag in sent:\n",
      "        if tag.startswith('N') and word in person_set:\n",
      "            person_nouns.append(word)\n",
      "            \n",
      "nltk.FreqDist(person_nouns).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 194,
       "text": [
        "[('candidate', 901),\n",
        " ('success', 778),\n",
        " ('life', 696),\n",
        " ('leader', 625),\n",
        " ('user', 458),\n",
        " ('partner', 365),\n",
        " ('expert', 220),\n",
        " ('face', 161),\n",
        " ('player', 156),\n",
        " ('case', 149),\n",
        " ('engineer', 125),\n",
        " ('machine', 118),\n",
        " ('image', 114),\n",
        " ('party', 104),\n",
        " ('transfer', 93),\n",
        " ('subject', 88),\n",
        " ('authority', 75),\n",
        " ('applicant', 72),\n",
        " ('relation', 63),\n",
        " ('self', 58),\n",
        " ('sort', 58),\n",
        " ('owner', 54),\n",
        " ('prospect', 48),\n",
        " ('worker', 42),\n",
        " ('adult', 36),\n",
        " ('learner', 36),\n",
        " ('advocate', 32),\n",
        " ('counter', 32),\n",
        " ('appointment', 24),\n",
        " ('peer', 24),\n",
        " ('Balance', 21),\n",
        " ('Latin', 19),\n",
        " ('junior', 19),\n",
        " ('hope', 18),\n",
        " ('communicator', 16),\n",
        " ('child', 15),\n",
        " ('cashier', 13),\n",
        " ('intellect', 13),\n",
        " ('quarter', 12),\n",
        " ('Cancer', 10),\n",
        " ('traveler', 9),\n",
        " ('walk-in', 9),\n",
        " ('fiduciary', 8),\n",
        " ('scientist', 8),\n",
        " ('controller', 7),\n",
        " ('achiever', 6),\n",
        " ('creator', 6),\n",
        " ('interpreter', 6),\n",
        " ('native', 6),\n",
        " ('threat', 6),\n",
        " ('Ram', 5),\n",
        " ('active', 5),\n",
        " ('female', 5),\n",
        " ('mediocrity', 5),\n",
        " ('mouse', 5),\n",
        " ('sport', 5),\n",
        " ('match', 4),\n",
        " ('planner', 4),\n",
        " ('technologist', 4),\n",
        " ('Twin', 3),\n",
        " ('agnostic', 3),\n",
        " ('birth', 3),\n",
        " ('collector', 3),\n",
        " ('man', 3),\n",
        " ('visionary', 3),\n",
        " ('witness', 3),\n",
        " ('Archer', 2),\n",
        " ('Black', 2),\n",
        " ('baby', 2),\n",
        " ('enrollee', 2),\n",
        " ('fastener', 2),\n",
        " ('gatekeeper', 2),\n",
        " ('observer', 2),\n",
        " ('precursor', 2),\n",
        " ('shaker', 2),\n",
        " ('showman', 2),\n",
        " ('winner', 2),\n",
        " ('Fish', 1),\n",
        " ('Gemini', 1),\n",
        " ('beard', 1),\n",
        " ('debtor', 1),\n",
        " ('double', 1),\n",
        " ('extrovert', 1),\n",
        " ('manipulator', 1),\n",
        " ('neighbor', 1),\n",
        " ('orphan', 1),\n",
        " ('relative', 1),\n",
        " ('selector', 1),\n",
        " ('signatory', 1),\n",
        " ('victim', 1),\n",
        " ('ward', 1),\n",
        " ('watcher', 1)]"
       ]
      }
     ],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize_chunks(chunk_list):\n",
      "    '''Removes punctuation and makes words lowercase'''\n",
      "    normed_chunks = []\n",
      "    for chunk in chunk_list:\n",
      "        new_chunk = []\n",
      "        for word, tag in chunk:\n",
      "            if word not in string.punctuation:\n",
      "                word = word.lower()\n",
      "                new_chunk.append((word,tag))\n",
      "        normed_chunks.append(new_chunk)\n",
      "    return normed_chunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 195
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extracting personal characteristics chunks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag='NP'\n",
      "grammar = r'''\n",
      "        NP:\n",
      "            {<JJ>(<,>?<JJ><,>?)*(<CC>?<JJ|V.*|AT|IN|CD>*)*<N.*>+} # noun phrase with adjectives, optionally containing prepositions, verbs or cardinal numbers\n",
      "        '''\n",
      "\n",
      "\n",
      "def get_personal_chunks(sentences, tag=tag, grammar=grammar):\n",
      "    ''''''\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    chunks = []\n",
      "    for sent in sentences:\n",
      "        sent_string = ' '.join(word[0] for word in sent)\n",
      "        if ('candidate' in sent_string or 'applicant' in sent_string or 'experience' in sent_string or ('we' and 'looking') in sent_string or 'skill' in sent_string) and ('environment' not in sent_string and 'place' not in sent_string and 'company' not in sent_string and 'employer' not in sent_string and not 'place' in sent_string and 'setting' not in sent_string and 'national' not in sent_string):\n",
      "            tree = cp.parse(sent)\n",
      "            for subtree in tree.subtrees():\n",
      "                if subtree.node == tag: \n",
      "                    chunks.append(subtree.leaves())\n",
      "    return normalize_chunks(chunks)\n",
      "\n",
      "personal_chunks = get_personal_chunks(tagged_sents_flat)\n",
      "\n",
      "test_chunks = []\n",
      "for result in personal_chunks:\n",
      "    sent_string = ' '.join(word[0] for word in result)\n",
      "    test_chunks.append(sent_string)\n",
      "\n",
      "def get_personal_list(input_chunks):\n",
      "    personal_chunks = input_chunks\n",
      "    chunks = []\n",
      "    for result in personal_chunks:\n",
      "        sent_string = ' '.join(word[0] for word in result)\n",
      "        chunk = [word[0] for word in result]\n",
      "        chunks.append(chunk)\n",
      "    return chunks    \n",
      "\n",
      "nltk.FreqDist(test_chunks).items()[:40]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 196,
       "text": [
        "[('ideal candidate', 232),\n",
        " ('successful candidate', 166),\n",
        " ('excellent communication skills', 79),\n",
        " ('equivalent experience', 74),\n",
        " ('interpersonal skills', 71),\n",
        " ('extraordinary customer experience', 61),\n",
        " ('reasonable accommodation:', 61),\n",
        " ('strong communication', 59),\n",
        " ('excellent customer service experience', 50),\n",
        " ('strong communication skills', 49),\n",
        " ('new career', 48),\n",
        " ('excellent verbal and written communication skills', 46),\n",
        " ('extensive experience', 44),\n",
        " ('excellent communication', 40),\n",
        " ('organizational skills', 37),\n",
        " ('previous experience', 37),\n",
        " ('capable mentors', 35),\n",
        " ('valuable business skills', 35),\n",
        " ('equivalent combination', 34),\n",
        " ('excellent written and verbal communication skills', 33),\n",
        " ('relevant experience', 33),\n",
        " ('marketable skills', 32),\n",
        " ('strong interpersonal skills', 32),\n",
        " ('competitive salary', 30),\n",
        " ('ideal candidates', 30),\n",
        " ('comfortable experience', 27),\n",
        " ('exceptional experience', 27),\n",
        " ('long list', 27),\n",
        " ('analytical skills', 26),\n",
        " ('great opportunity', 26),\n",
        " ('consistent with the requirements', 25),\n",
        " ('early retirement', 25),\n",
        " ('new home', 25),\n",
        " ('unique experience', 25),\n",
        " ('criminal histories', 24),\n",
        " ('strong organizational skills', 24),\n",
        " ('strong experience', 22),\n",
        " ('basic cooking skills', 20),\n",
        " ('equivalent work experience', 20),\n",
        " ('excellent organizational skills', 20)]"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extracting work environment chunks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag='NP'\n",
      "grammar = r'''\n",
      "        NP:\n",
      "            {<JJ>(<,><JJ><,>?)*(<CC>?<JJ|V.*|AT|IN|CD>*)*<N.*>+} # noun phrase with adjectives, optionally containing prepositions, verbs or cardinal numbers\n",
      "        '''\n",
      "# After many different attempts, using same grammar as for personal characteristics produced the best results\n",
      "\n",
      "\n",
      "def get_environ_chunks(sentences, tag=tag, grammar=grammar):\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    chunks = []\n",
      "    for sent in sentences:\n",
      "        sent_string = ' '.join(word[0] for word in sent)\n",
      "        if ('environment' in sent_string or 'work' in sent_string or 'place' in sent_string or 'team' in sent_string or 'setting' in sent_string or ('you' and 'looking') in sent_string) and ('skill' not in sent_string and 'candidate' not in sent_string and ('we' and 'looking') not in sent_string and 'manner' not in sent_string):\n",
      "            tree = cp.parse(sent)\n",
      "            for subtree in tree.subtrees():\n",
      "                if subtree.node == tag:\n",
      "                    chunks.append(subtree.leaves())\n",
      "    return normalize_chunks(chunks)\n",
      "\n",
      "environ_chunks = get_environ_chunks(tagged_sents_flat)\n",
      "\n",
      "test_chunks = []\n",
      "for result in environ_chunks:\n",
      "    sent_string = ' '.join(word[0] for word in result)\n",
      "    test_chunks.append(sent_string)\n",
      "\n",
      "def get_environ_list(input_chunks):\n",
      "    environ_chunks = input_chunks\n",
      "    chunks = []\n",
      "    for result in environ_chunks:\n",
      "        sent_string = ' '.join(word[0] for word in result)\n",
      "        chunk = [word[0] for word in result]\n",
      "        chunks.append(chunk)\n",
      "    return chunks   \n",
      "\n",
      "nltk.FreqDist(test_chunks).items()[:40]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "[('federal law', 171),\n",
        " ('competitive compensation package', 152),\n",
        " ('immediate tuition reimbursement', 148),\n",
        " ('medical dental vision care', 148),\n",
        " ('supplemental life insurance benefits', 148),\n",
        " ('medical dental vision', 71),\n",
        " ('flexible hours', 69),\n",
        " ('diverse and inclusive work environment', 63),\n",
        " ('full benefits package', 63),\n",
        " ('fast paced team environment', 62),\n",
        " ('slow down-', 62),\n",
        " ('successful innovation', 62),\n",
        " ('full of fast-paced fun', 61),\n",
        " ('great pay', 61),\n",
        " ('retail opportunities', 61),\n",
        " ('strong benefits', 61),\n",
        " ('unlimited earning potential', 61),\n",
        " ('great place', 60),\n",
        " ('positive work environment', 48),\n",
        " ('fast paced environment', 45),\n",
        " ('flexible schedule', 43),\n",
        " ('hard work', 41),\n",
        " ('great opportunity', 33),\n",
        " ('responsible for p', 32),\n",
        " ('strong work ethic', 32),\n",
        " ('competitive salary', 31),\n",
        " ('new products', 31),\n",
        " ('high level', 28),\n",
        " ('internal teams', 27),\n",
        " ('genuine respect', 26),\n",
        " ('functional teams', 24),\n",
        " ('professional and courteous relationship', 24),\n",
        " ('strong working relationships', 24),\n",
        " ('minimal supervision', 23),\n",
        " ('strong team', 23),\n",
        " ('general liability', 22),\n",
        " (\"new employee 's i-9\", 22),\n",
        " ('commercial automobile', 21),\n",
        " ('equivalent work experience', 21),\n",
        " ('multiple departments', 21)]"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Utility functions for extracting nouns, verbs and adjectives (to display in interface)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getNouns(chunks):\n",
      "    nouns = []\n",
      "    for chunk in chunks:\n",
      "        for word, tag in chunk:\n",
      "            if tag.startswith('N'):\n",
      "                if wn.morphy(word.lower(), wn.NOUN): # if the lemma exists in wordnet\n",
      "                    word = wn.morphy(word.lower(), wn.NOUN)\n",
      "                nouns.append(word)\n",
      "    return nouns\n",
      "\n",
      "def getAdjs(chunks):\n",
      "    adjs = []\n",
      "    for chunk in chunks:\n",
      "        for word, tag in chunk:\n",
      "            if tag.startswith('J'):\n",
      "                if wn.morphy(word.lower(), wn.ADJ): # if the lemma exists in wordnet\n",
      "                    word = wn.morphy(word.lower(), wn.ADJ)\n",
      "                adjs.append(word)\n",
      "    return adjs\n",
      "\n",
      "def getVerbs(chunks):\n",
      "    verbs = []\n",
      "    for chunk in chunks:\n",
      "        for word, tag in chunk:\n",
      "            if tag.startswith('V'):\n",
      "                if wn.morphy(word.lower(), wn.VERB): # if the lemma exists in wordnet\n",
      "                    word = wn.morphy(word.lower(), wn.VERB)\n",
      "                verbs.append(word)\n",
      "    return verbs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 198
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Utility functions for extracting bigrams and trigrams from chunks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop_words = nltk.corpus.stopwords.words('english')\n",
      "to_remove = stop_words\n",
      "\n",
      "def getBigrams(chunk_list):\n",
      "    all_bigrams = [nltk.bigrams(act) for act in chunk_list]\n",
      "    all_bigrams_list = []\n",
      "    for bigrams in all_bigrams:\n",
      "        [all_bigrams_list.append(bigram) for bigram in bigrams if bigram[0] not in to_remove and bigram[1] not in to_remove]\n",
      "    return all_bigrams_list\n",
      "\n",
      "def getTrigrams(chunk_list):\n",
      "    all_trigrams = [nltk.trigrams(act) for act in chunk_list]\n",
      "    all_trigrams_list = []\n",
      "    for trigrams in all_trigrams:\n",
      "        [all_trigrams_list.append(trigram) for trigram in trigrams if trigram[0] not in to_remove and trigram[1] not in to_remove and trigram[2] not in to_remove]\n",
      "    return all_trigrams_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 199
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create separate dataframes for environment and personal characteristics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "environ_df = data.copy()\n",
      "personal_df = data.copy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "environ_df['work_environment'] = environ_df.ngram_tags.map(get_environ_chunks)\n",
      "\n",
      "environ_df.work_environment.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 201,
       "text": [
        "DID\n",
        "JHL4PY5YCYWRKR5SR63    [[(genuine, JJ), (passion, NN)], [(professiona...\n",
        "J3H6HZ6HJ4RMVWCG7HZ    [[(dynamic, JJ), (learning, NN)], [(responsibl...\n",
        "J3H3BC6PGK27KG7YZLN    [[(solar, JJ), (installers, NN)], [(dynamic, J...\n",
        "JHR71P6BGWQYFCSN5ZW                                                   []\n",
        "J3H5FV6N2RZXBQKJ2ZZ    [[(wide, JJ), (range, NN)], [(financial, JJ), ...\n",
        "Name: work_environment, dtype: object"
       ]
      }
     ],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "environ_df['environ_nouns'] = environ_df.work_environment.map(getNouns)\n",
      "environ_df['environ_adjs'] = environ_df.work_environment.map(getAdjs)\n",
      "environ_df['environ_verbs'] = environ_df.work_environment.map(getVerbs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "environ_df['environ_list'] = environ_df.work_environment.map(get_environ_list)\n",
      "environ_df['environ_string'] = environ_df.environ_list.map(lambda x: [' '.join(sublist) for sublist in x])\n",
      "\n",
      "environ_df.environ_string.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 203,
       "text": [
        "DID\n",
        "JHL4PY5YCYWRKR5SR63    [genuine passion, professional excellence, hig...\n",
        "J3H6HZ6HJ4RMVWCG7HZ    [dynamic learning, responsible for developing ...\n",
        "J3H3BC6PGK27KG7YZLN    [solar installers, dynamic environment withmul...\n",
        "JHR71P6BGWQYFCSN5ZW                                                   []\n",
        "J3H5FV6N2RZXBQKJ2ZZ    [wide range, financial and decision supportinf...\n",
        "Name: environ_string, dtype: object"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "environ_df['environ_bigrams'] = environ_df.environ_list.map(getBigrams)\n",
      "environ_df['environ_trigrams'] = environ_df.environ_list.map(getTrigrams)\n",
      "\n",
      "environ_df.environ_bigrams.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 204,
       "text": [
        "DID\n",
        "JHL4PY5YCYWRKR5SR63    [(genuine, passion), (professional, excellence...\n",
        "J3H6HZ6HJ4RMVWCG7HZ    [(dynamic, learning), (facilitating, training)...\n",
        "J3H3BC6PGK27KG7YZLN    [(solar, installers), (dynamic, environment), ...\n",
        "JHR71P6BGWQYFCSN5ZW                                                   []\n",
        "J3H5FV6N2RZXBQKJ2ZZ    [(wide, range), (decision, supportinformation)...\n",
        "Name: environ_bigrams, dtype: object"
       ]
      }
     ],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "work_environment = environ_df.iloc[:,6:] # don't need first few columns anymore\n",
      "# work_environment.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 205
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Output data pickle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "work_environment.to_pickle(path.join(pardir, 'data','work_environment_dec16.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 206
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Personal Characteristics dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_df['personal_char'] = personal_df.ngram_tags.map(get_personal_chunks)\n",
      "# personal_df.personal_char"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_df['personal_nouns'] = personal_df.personal_char.map(getNouns)\n",
      "personal_df['personal_adjs'] = personal_df.personal_char.map(getAdjs)\n",
      "personal_df['personal_verbs'] = personal_df.personal_char.map(getVerbs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 208
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_df.personal_adjs.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": [
        "DID\n",
        "JHL4PY5YCYWRKR5SR63    [current, direct, technical, multiple, direct,...\n",
        "J3H6HZ6HJ4RMVWCG7HZ    [primary, new, soft, effective, excellent, ver...\n",
        "J3H3BC6PGK27KG7YZLN                      [successful, excellent, verbal]\n",
        "JHR71P6BGWQYFCSN5ZW                          [low, available, realistic]\n",
        "J3H5FV6N2RZXBQKJ2ZZ    [effective, ideal, superior, financial, prior,...\n",
        "Name: personal_adjs, dtype: object"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_df['personal_list'] = personal_df.personal_char.map(get_personal_list)\n",
      "personal_df['personal_string'] = personal_df.personal_list.map(lambda x: [' '.join(sublist) for sublist in x])\n",
      "\n",
      "personal_df.personal_string.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 210,
       "text": [
        "DID\n",
        "JHL4PY5YCYWRKR5SR63    [current experience, direct sales reps, techni...\n",
        "J3H6HZ6HJ4RMVWCG7HZ    [primary responsibilities, new hire orientatio...\n",
        "J3H3BC6PGK27KG7YZLN    [successful experience, excellent verbal and w...\n",
        "JHR71P6BGWQYFCSN5ZW    [low start-up investment, available for qualif...\n",
        "J3H5FV6N2RZXBQKJ2ZZ    [effective communicators, ideal candidate, sup...\n",
        "Name: personal_string, dtype: object"
       ]
      }
     ],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_df['personal_bigrams'] = personal_df.personal_list.map(getBigrams)\n",
      "personal_df['personal_trigrams'] = personal_df.personal_list.map(getTrigrams)\n",
      "# personal_df.personal_trigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_char = personal_df.iloc[:,6:]\n",
      "# personal_char.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 212
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Output data pickle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personal_char.to_pickle(path.join(pardir, 'data','personal_char_dec16.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    }
   ],
   "metadata": {}
  }
 ]
}